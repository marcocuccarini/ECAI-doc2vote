{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a45641-296d-41cd-8e3d-d5eb8e4be56f",
   "metadata": {},
   "source": [
    "# Notebook for reproduce experiments of the paper doc2vote paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "706b8961-bb4c-4aee-bce2-bf40ef133fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.dataset import Dataset\n",
    "from classes.evaluator import Evaluator\n",
    "from classes.model import BM25\n",
    "\n",
    "import ir_datasets\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e4f67e7-0849-4030-a529-324280427af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(path):\n",
    "\n",
    "\t with open(path) as f:\n",
    "\n",
    "\t \td = json.load(f)\n",
    "\t \treturn d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb9d9ce-2ff5-4bed-b52f-6a3be6e9b44d",
   "metadata": {},
   "source": [
    "### Create the dataset instance and initialize the attributes of the reduced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43bb43e9-6b15-4ff1-b127-7cc01f66a3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc161a3bc944b6eb5e3f59765f53ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_n=10\n",
    "\n",
    "\n",
    "dataset_config = extract_json(\"/Users/marco/Documents/GitHub/ECAI-doc2vote/code/configuration/configDatasetMarco.json\")\n",
    "\n",
    "dataset=Dataset(dataset_config['dataset_name_test_2'])\n",
    "#extarct all the passage of the dataset reduce\n",
    "dataset.list_raduce_passage()\n",
    "#add all the augmented file, it is applied a filter becouse it in not able\n",
    "#to create a dictionary with the complete dataset\n",
    "dataset.add_augmented_file()\n",
    "#create the feature \n",
    "dataset.raduce_passage()\n",
    "#join togheter the passage base and the passage augmented \n",
    "dataset.join_passage_base_aug(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e748f5e-0bbf-4ac9-9a3f-980fe44e5be6",
   "metadata": {},
   "source": [
    "### Test for classical test with model BM25 on the reduce dataset MSMarco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a50aaea0-5b9f-4a4d-bc9e-2f24ae309cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model = BM25(list(dataset.reduce_passages.values()))\\n\\nscore, dict_prediction = model.ranking_score(dataset.query, dataset.reduce_passages, top_n)\\n\\nevaluator= Evaluator(dict_prediction,dataset.query_passage)\\n\\nrank=evaluator.extract_ranking(top_n)\\n\\nprint(\"BM25\", evaluator.mean_reciprocal_rank(list(rank.values())))'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model = BM25(list(dataset.reduce_passages.values()))\n",
    "\n",
    "score, dict_prediction = model.ranking_score(dataset.query, dataset.reduce_passages, top_n)\n",
    "\n",
    "evaluator= Evaluator(dict_prediction,dataset.query_passage)\n",
    "\n",
    "rank=evaluator.extract_ranking(top_n)\n",
    "\n",
    "print(\"BM25\", evaluator.mean_reciprocal_rank(list(rank.values())))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740ead93-1310-4155-9935-ed352df2c63c",
   "metadata": {},
   "source": [
    "### Test for doc2vote test with model BM25 on the reduce dataset MSMarco "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "667a604b-9ee3-475f-bf59-9f148192a581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 with doc2vec: 0.9375732750469962\n"
     ]
    }
   ],
   "source": [
    "model = BM25(list(dataset.reduce_passages_base_aug.values()))\n",
    "\n",
    "\n",
    "score, dict_prediction = model.ranking_score(dataset.query, dataset.reduce_passages_base_aug, top_n)\n",
    "\n",
    "evaluator= Evaluator(dict_prediction,dataset.query_passage)\n",
    "\n",
    "rank=evaluator.extract_ranking(top_n)\n",
    "\n",
    "\n",
    "print(\"BM25 with doc2vec:\", evaluator.mean_reciprocal_rank(list(rank.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9118a547-2eb9-47c5-8908-df120a9f99a4",
   "metadata": {},
   "source": [
    "### Test for doc2vote test with model BM25 on the reduce dataset MSMarco indipendendent from position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d182a02e-38a6-4352-a3b2-2f1b0d45ebbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'top_n=10\\naug_num=20\\ndataset.passage_creation_vote(aug_num)\\n\\nlist_passages=[x[1] for x in dataset.list_doc2vote]\\nindex_passages=[x[0] for x in dataset.list_doc2vote]\\n\\nmodel = BM25(list(list_passages))\\n\\nscore, dict_prediction = model.ranking_score_partial(dataset.query, list_passages, index_passages, top_n*aug_num)\\n\\nfrom collections import Counter\\ncounts_per_id = {\\n    key: dict(sorted(Counter(value).items(), key=lambda item: item[1], reverse=True))\\n    for key, value in dict_prediction.items()\\n}\\n\\ndict_new={}\\nfor j in counts_per_id.keys():\\n    \\n    dict_new[j]=counts_per_id[j].keys()\\n\\nevaluator= Evaluator(dict_new,dataset.query_passage)\\n\\nrank=evaluator.extract_ranking(10)\\n\\nprint(\"BM25 with doc2vote:\", evaluator.mean_reciprocal_rank(list(rank.values())))'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''top_n=10\n",
    "aug_num=20\n",
    "dataset.passage_creation_vote(aug_num)\n",
    "\n",
    "list_passages=[x[1] for x in dataset.list_doc2vote]\n",
    "index_passages=[x[0] for x in dataset.list_doc2vote]\n",
    "\n",
    "model = BM25(list(list_passages))\n",
    "\n",
    "score, dict_prediction = model.ranking_score_partial(dataset.query, list_passages, index_passages, top_n*aug_num)\n",
    "\n",
    "from collections import Counter\n",
    "counts_per_id = {\n",
    "    key: dict(sorted(Counter(value).items(), key=lambda item: item[1], reverse=True))\n",
    "    for key, value in dict_prediction.items()\n",
    "}\n",
    "\n",
    "dict_new={}\n",
    "for j in counts_per_id.keys():\n",
    "    \n",
    "    dict_new[j]=counts_per_id[j].keys()\n",
    "\n",
    "evaluator= Evaluator(dict_new,dataset.query_passage)\n",
    "\n",
    "rank=evaluator.extract_ranking(10)\n",
    "\n",
    "print(\"BM25 with doc2vote:\", evaluator.mean_reciprocal_rank(list(rank.values())))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a923081-cf4d-45dd-8b2c-19ae31418e06",
   "metadata": {},
   "source": [
    "### Test for doc2vote test with model BM25 on the reduce dataset MSMarco dipendent from position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd23c1e-a37a-4079-8ed6-4574fee0f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "top_n = 10\n",
    "aug_num = 40\n",
    "dataset.passage_creation_vote(aug_num)\n",
    "\n",
    "list_passages = [x[1] for x in dataset.list_doc2vote]\n",
    "index_passages = [x[0] for x in dataset.list_doc2vote]\n",
    "\n",
    "model = BM25(list(list_passages))\n",
    "\n",
    "score, dict_prediction = model.ranking_score_partial(dataset.query, list_passages, index_passages, top_n * aug_num)\n",
    "\n",
    "# Weighted vote counting\n",
    "counts_per_id = {}\n",
    "\n",
    "for key, value in dict_prediction.items():\n",
    "    weighted_votes = {}\n",
    "    \n",
    "    for rank, passage_id in enumerate(value, start=1):  # Rank starts from 1\n",
    "        weight = 1 / rank  # Reciprocal rank weighting (can be changed)\n",
    "        weighted_votes[passage_id] = weighted_votes.get(passage_id, 0) + weight\n",
    "    \n",
    "    # Sort by weighted votes in descending order\n",
    "    counts_per_id[key] = dict(sorted(weighted_votes.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Extract ranked lists\n",
    "dict_new = {j: list(counts_per_id[j].keys()) for j in counts_per_id.keys()}\n",
    "\n",
    "evaluator = Evaluator(dict_new, dataset.query_passage)\n",
    "\n",
    "rank = evaluator.extract_ranking(10)\n",
    "\n",
    "print(\"BM25 with weighted doc2vote:\", evaluator.mean_reciprocal_rank(list(rank.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8625eef6-eba3-4e16-bc07-cd9e897bd3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "top_n = 10\n",
    "aug_num = 40\n",
    "dataset.passage_creation_vote(aug_num)\n",
    "\n",
    "list_passages = [x[1] for x in dataset.list_doc2vote]\n",
    "index_passages = [x[0] for x in dataset.list_doc2vote]\n",
    "\n",
    "model = BM25(list(list_passages))\n",
    "\n",
    "score, dict_prediction = model.ranking_score_partial(dataset.query, list_passages, index_passages, top_n * aug_num)\n",
    "\n",
    "# Weighted vote counting\n",
    "counts_per_id = {}\n",
    "\n",
    "for key, value in dict_prediction.items():\n",
    "    weighted_votes = {}\n",
    "    \n",
    "    for rank, passage_id in enumerate(value, start=1):  # Rank starts from 1\n",
    "        weight = score[passage_id] # Reciprocal rank weighting (can be changed)\n",
    "        weighted_votes[passage_id] = weighted_votes.get(passage_id, 0) + weight\n",
    "    \n",
    "    # Sort by weighted votes in descending order\n",
    "    counts_per_id[key] = dict(sorted(weighted_votes.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Extract ranked lists\n",
    "dict_new = {j: list(counts_per_id[j].keys()) for j in counts_per_id.keys()}\n",
    "\n",
    "evaluator = Evaluator(dict_new, dataset.query_passage)\n",
    "\n",
    "rank = evaluator.extract_ranking(10)\n",
    "\n",
    "print(\"BM25 with weighted doc2vote:\", evaluator.mean_reciprocal_rank(list(rank.values())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
